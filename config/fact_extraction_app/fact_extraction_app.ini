#
# fact_extraction_app config
#

[datasets]

# note: define either an input_file OR input_folder.

# the input_folder will be polled and new files read in (empty string for no folder).
# a OS specific move command should be used to avoid partially written files in this polling folder.
# once moved to polling dir files should not be subsequently updated as the file change date is used to decide if a file is new or not.
input_folder=/projects-git/intel-analysis-dstl/fact-extraction/build/input_folder_dynamic
#input_folder=

# polling frequency in seconds (will load all files that has a more recent filestamp than last check)
input_folder_polling_freq=1

# squirrel city scenario (empty string for no folder)
#input_file=/projects-git/intel-analysis-dstl/fact-extraction/corpus/phase1-scenario/squirrel_city_tweets_repurposed_short.json
#input_file=/projects-git/intel-analysis-dstl/fact-extraction/corpus/phase1-scenario/squirrel_city_tweets_repurposed_v2.json
input_file=

# output file(s)
output_file_pos=pos_labelled_sents.txt
output_file_seeds=seed_tuples.txt
output_file_extraction=extracted_entities.txt

# training corpus
#training_input_file=/projects-git/intel-analysis-dstl/fact-extraction/corpus/phase1-scenario/squirrel_city_tweets_repurposed_short.json
training_input_file=/projects-git/intel-analysis-dstl/fact-extraction/corpus/phase1-scenario/squirrel_city_tweets_repurposed_v2.json
open_extraction_templates_file=extraction_templates.txt

[unit-test]

# limit for number of sents before processing stops (-1 for no limit) - note sents processed in text blocks so might get a few extra
max_sent_limit=-1

# random sample of sentences used for creating open extraction templates
# note: this needs to be big enough to capture important lexical terms, but small enough the templates can be generated in a reasonable timeframe (e.g. 1000 takes about 24h to generate on Berton)
random_subset_training=500

# topN open extraction template rules to keep
# note: this needs to be big enough to capture the most important linguistic patterns (e.g. 1000) BUT not so large it takes 5 minutes to execute the patterns on each sentence (e.g. 100,000)
# 20,000 templates take about 1 minute to execute per sentence on Berton (which is a problem is you have 8000 sents)
target_extraction_templates=1000

# number of processes to spawn for CPU intensive NLP processes (generate open extraction templates, extraction using templates)
# matching to number of available CPU cores is probably optimal
process_count=4

# if true app will publish a vocab message to get things started (disable if connected to CISpaces UI) and delete any existing database for a clean run
unit_test_publish_vocab=True

[common]

# language code to use
language_codes=['en']

# stanford tagger dir
stanford_tagger_dir=/stanford-postagger-full

# stanford dependancy parser dir
stanford_parser_dir=/stanford-parser-full

# parser model
model_path=edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz
model_jar=/stanford-parser-full/stanford-english-corenlp-2016-10-31-models.jar
model_options=

[situation-assessment]

# assessment id (will be prefixed to make postgresql table names)
assessment_id_post=phase1_post
assessment_id_ext=phase1_ext

# database details
db_user=postgres
db_pass=postgres
db_host=localhost
db_port=5432
db_name=intelanalysis
db_schema=factextract

[table-specs]
item_table_suffix=item
index_table_suffix=index

# assessment_id length limit (to avoid generating PostgreSQL table names over limit)
max_assess_id_length = 14

# database specific maximum table name limit
max_table_length = 63

table_specs_post=[
	{
		'table_type': 'item',
		'column_spec': [
			['item_key', 'serial NOT NULL'],
			['source_uri', 'text UNIQUE'],
			['created_at', 'timestamp with time zone', None],
			['text', 'text', ''],
			['updated_time', 'timestamp with time zone', 'current_timestamp'] # default value 'current_timestamp' gets current time of insert transaction
		],
		'index_cols': ['source_uri']
	}
	]

table_specs_ext=[
	{
		'table_type': 'item',
		'column_spec': [
			['item_key', 'serial NOT NULL'],
			['extract_uri', 'text UNIQUE'],
			['source_uri', 'text'],
			['extract', 'text'],
			['encoded', 'text'],
			['updated_time', 'timestamp with time zone', 'current_timestamp'] # default value 'current_timestamp' gets current time of insert transaction
		],
		'index_cols': ['extract_uri']
	},
	{
		'table_type': 'annotation',
		'annotation_label': 'topic',
		'column_spec': [
			['topic_key', 'serial NOT NULL'],
			['topic', 'text'],
			['schema', 'text'],
			['negated', 'boolean'],
			['genuine', 'boolean']
		],
		'index_cols': ['topic', 'schema', 'negated', 'genuine']
	}
	]

[rabbitmq]

rmq_host=localhost
rmq_port=5672
rmq_username=guest
rmq_password=guest
rmq_queue_name=
rmq_timeout_statement=60
rmq_timeout_connection=120
rmq_timeout_overall=120

# connection parameters (java + python)
# heartbeat to keep alive, 5 retries on a bad connection waiting 10 seconds between them
rmq_connection_parameters_java=connection_attempts=3&retry_delay=1
rmq_connection_parameters_python=[('connection_attempts',3),('retry_delay',1)]

# rmq queue settings
rmq_exclusive_queue=True
rmq_auto_delete_queue=True

# rmq exchange settings
rmq_exchange_type=fanout
rmq_exchange_name=cispaces_exchange

# rmq routing
rmq_routing=cispaces_routing
